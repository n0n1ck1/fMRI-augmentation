{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLoCilr3z7xe",
        "outputId": "f10cdb5c-d0d1-418f-acd5-7d72e111a1d0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import abstractmethod\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import math\n",
        "from typing import Optional\n",
        "import warnings\n",
        "\n",
        "class BaseAttention(nn.Module):\n",
        "    @abstractmethod\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, x, context=None, mask=None):\n",
        "        pass\n",
        "\n",
        "def scaled_multihead_dot_product_attention(\n",
        "    query,\n",
        "    key,\n",
        "    value,\n",
        "    heads,\n",
        "    past_key_value=None,\n",
        "    softmax_scale=None,\n",
        "    bias=None,\n",
        "    key_padding_mask=None,\n",
        "    causal=False,\n",
        "    dropout=0.0,\n",
        "    training=False,\n",
        "    needs_weights=False,\n",
        "    multiquery=False,\n",
        "):\n",
        "    q = rearrange(query, 'b s (h d) -> b h s d', h=heads)\n",
        "    kv_heads = 1 if multiquery else heads\n",
        "    k = rearrange(key, 'b s (h d) -> b h d s', h=kv_heads)\n",
        "    v = rearrange(value, 'b s (h d) -> b h s d', h=kv_heads)\n",
        "\n",
        "    if past_key_value is not None:\n",
        "        # attn_impl: flash & triton use kernels which expect input shape [b, s, h, d_head].\n",
        "        # kv_cache is therefore stored using that shape.\n",
        "        # attn_impl: torch stores the kv_cache in the ordering which is most advantageous\n",
        "        # for its attn computation ie\n",
        "        # keys are stored as tensors with shape [b, h, d_head, s] and\n",
        "        # values are stored as tensors with shape [b, h, s, d_head]\n",
        "        if len(past_key_value) != 0:\n",
        "            k = torch.cat([past_key_value[0], k], dim=3)\n",
        "            v = torch.cat([past_key_value[1], v], dim=2)\n",
        "\n",
        "        past_key_value = (k, v)\n",
        "\n",
        "    b, _, s_q, d = q.shape\n",
        "    s_k = k.size(-1)\n",
        "\n",
        "    if softmax_scale is None:\n",
        "        softmax_scale = 1 / math.sqrt(d)\n",
        "\n",
        "    attn_weight = q.matmul(k) * softmax_scale\n",
        "\n",
        "    if bias is not None:\n",
        "        # clamp to 0 necessary for torch 2.0 compile()\n",
        "        _s_q = max(0, bias.size(2) - s_q)\n",
        "        _s_k = max(0, bias.size(3) - s_k)\n",
        "        bias = bias[:, :, _s_q:, _s_k:]\n",
        "\n",
        "        if (bias.size(-1) != 1 and\n",
        "                bias.size(-1) != s_k) or (bias.size(-2) != 1 and\n",
        "                                               bias.size(-2) != s_q):\n",
        "            raise RuntimeError(\n",
        "                f'bias (shape: {bias.shape}) is expected to broadcast to shape: {attn_weight.shape}.'\n",
        "            )\n",
        "        attn_weight = attn_weight + bias\n",
        "\n",
        "    min_val = torch.finfo(q.dtype).min\n",
        "\n",
        "    if key_padding_mask is not None:\n",
        "        if bias is not None:\n",
        "            warnings.warn(\n",
        "                'Propogating key_padding_mask to the attention module ' +\\\n",
        "                'and applying it within the attention module can cause ' +\\\n",
        "                'unneccessary computation/memory usage. Consider integrating ' +\\\n",
        "                'into bias once and passing that to each attention ' +\\\n",
        "                'module instead.'\n",
        "            )\n",
        "        attn_weight = attn_weight.masked_fill(\n",
        "            ~key_padding_mask.view((b, 1, 1, s_k)), min_val)\n",
        "\n",
        "    if causal and (not q.size(2) == 1):\n",
        "        s = max(s_q, s_k)\n",
        "        causal_mask = attn_weight.new_ones(s, s, dtype=torch.float32)\n",
        "        causal_mask = causal_mask.tril()\n",
        "        causal_mask = causal_mask.to(torch.bool)\n",
        "        causal_mask = ~causal_mask\n",
        "        causal_mask = causal_mask[-s_q:, -s_k:]\n",
        "        attn_weight = attn_weight.masked_fill(causal_mask.view(1, 1, s_q, s_k),\n",
        "                                              min_val)\n",
        "\n",
        "    attn_weight = torch.softmax(attn_weight, dim=-1)\n",
        "\n",
        "    if dropout:\n",
        "        attn_weight = torch.nn.functional.dropout(attn_weight,\n",
        "                                                  p=dropout,\n",
        "                                                  training=training,\n",
        "                                                  inplace=True)\n",
        "\n",
        "    out = attn_weight.to(v.dtype).matmul(v)\n",
        "    out = rearrange(out, 'b h s d -> b s (h d)')\n",
        "\n",
        "    if needs_weights:\n",
        "        return out, attn_weight, past_key_value\n",
        "    return out, None, past_key_value\n",
        "\n",
        "class MultiQueryAttention(BaseAttention):\n",
        "    \"\"\"Multi-Query self attention.\n",
        "\n",
        "    Using torch or triton attention implemetation enables user to also use\n",
        "    additive bias.\n",
        "\n",
        "    Look for documentation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        heads: int,\n",
        "        attn_impl: str = \"torch\",\n",
        "        clip_qkv: Optional[float] = None,\n",
        "        qk_ln: bool = False,\n",
        "        softmax_scale: Optional[float] = None,\n",
        "        attn_pdrop: float = 0.0,\n",
        "        norm_type: str = \"low_precision_layernorm\",\n",
        "        fc_type: str = \"torch\",\n",
        "        verbose: int = 0,\n",
        "        device: Optional[str] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attn_impl = attn_impl\n",
        "        self.clip_qkv = clip_qkv\n",
        "        self.qk_ln = qk_ln\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.heads = heads\n",
        "        self.head_dim = d_model // heads\n",
        "        self.softmax_scale = softmax_scale\n",
        "        if self.softmax_scale is None:\n",
        "            self.softmax_scale = 1 / math.sqrt(self.head_dim)\n",
        "        self.attn_dropout = attn_pdrop\n",
        "\n",
        "        fc_kwargs = {}\n",
        "        if fc_type != \"te\":\n",
        "            fc_kwargs[\"device\"] = device\n",
        "        # - vchiley\n",
        "        self.Wqkv = nn.Linear(\n",
        "            d_model,\n",
        "            d_model + 2 * self.head_dim,\n",
        "            **fc_kwargs,\n",
        "        )\n",
        "        # for param init fn; enables shape based init of fused layers\n",
        "        fuse_splits = (d_model, d_model + self.head_dim)\n",
        "        self.Wqkv._fused = (0, fuse_splits)  # type: ignore\n",
        "\n",
        "        if self.qk_ln:\n",
        "            norm_class = nn.LayerNorm\n",
        "            self.q_ln = norm_class(d_model, device=device)\n",
        "            self.k_ln = norm_class(self.head_dim, device=device)\n",
        "\n",
        "        self.attn_fn = scaled_multihead_dot_product_attention\n",
        "        if torch.cuda.is_available() and verbose:\n",
        "            warnings.warn(\n",
        "                \"Using `attn_impl: torch`. If your model does not use\"\n",
        "                \" `alibi` or \"\n",
        "                + \"`prefix_lm` we recommend using `attn_impl: flash`\"\n",
        "                \" otherwise \"\n",
        "                + \"we recommend using `attn_impl: triton`.\"\n",
        "            )\n",
        "\n",
        "\n",
        "        self.out_proj = nn.Linear(\n",
        "            self.d_model,\n",
        "            self.d_model,\n",
        "            **fc_kwargs,\n",
        "        )\n",
        "        self.out_proj._is_residual = True  # type: ignore\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x,\n",
        "        past_key_value=None,\n",
        "        bias=None,\n",
        "        mask=None,\n",
        "        causal=True,\n",
        "        needs_weights=False,\n",
        "    ):\n",
        "        qkv = self.Wqkv(x)\n",
        "\n",
        "        if self.clip_qkv:\n",
        "            qkv = qkv.clamp(min=-self.clip_qkv, max=self.clip_qkv)\n",
        "\n",
        "        query, key, value = qkv.split(\n",
        "            [self.d_model, self.head_dim, self.head_dim], dim=2\n",
        "        )\n",
        "\n",
        "        key_padding_mask = mask\n",
        "\n",
        "        if self.qk_ln:\n",
        "            # Applying layernorm to qk\n",
        "            dtype = query.dtype\n",
        "            query = self.q_ln(query).to(dtype)\n",
        "            key = self.k_ln(key).to(dtype)\n",
        "\n",
        "        context, attn_weights, past_key_value = self.attn_fn(\n",
        "            query,\n",
        "            key,\n",
        "            value,\n",
        "            self.heads,\n",
        "            past_key_value=past_key_value,\n",
        "            softmax_scale=self.softmax_scale,\n",
        "            bias=bias,\n",
        "            key_padding_mask=key_padding_mask,\n",
        "            causal=causal,\n",
        "            dropout=self.attn_dropout,\n",
        "            training=self.training,\n",
        "            needs_weights=needs_weights,\n",
        "            multiquery=True,\n",
        "        )\n",
        "\n",
        "        return self.out_proj(context), attn_weights, past_key_value"
      ],
      "metadata": {
        "id": "qqwtzivG5IgI"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn, Tensor\n",
        "from einops import rearrange, reduce\n",
        "\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(\n",
        "                hidden_dim, in_dim\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def threed_to_text(\n",
        "    x: Tensor, max_seq_len: int, dim: int, flatten: bool = False\n",
        "):\n",
        "    \"\"\"\n",
        "    Converts a 3D tensor to text representation.\n",
        "\n",
        "    Args:\n",
        "        x (Tensor): The input tensor of shape (batch_size, sequence_length, input_dim).\n",
        "        max_seq_len (int): The maximum sequence length of the output tensor.\n",
        "        dim (int): The dimension of the intermediate tensor.\n",
        "        flatten (bool, optional): Whether to flatten the intermediate tensor. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: The output tensor of shape (batch_size, max_seq_len, input_dim).\n",
        "    \"\"\"\n",
        "    b, s, d = x.shape\n",
        "\n",
        "    x = nn.Linear(d, dim)(x)\n",
        "\n",
        "    x = rearrange(x, \"b s d -> b d s\")\n",
        "    x = nn.Linear(s, max_seq_len)(x)\n",
        "    x = rearrange(x, \"b d s -> b s d\")\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "def scatter3d_to_4d_spatial(x: Tensor, dim: int):\n",
        "    \"\"\"\n",
        "    Scatters a 3D tensor into a 4D spatial tensor using einops.\n",
        "\n",
        "    Args:\n",
        "        x (Tensor): The input tensor of shape (b, s, d), where b is the batch size, s is the spatial dimension, and d is the feature dimension.\n",
        "        dim (int): The dimension along which to scatter the tensor.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: The scattered 4D spatial tensor of shape (b, (s*s1), d), where s1 is the new spatial dimension after scattering.\n",
        "    \"\"\"\n",
        "    b, s, d = x.shape\n",
        "\n",
        "    # Scatter the 3D tensor into a 4D spatial tensor\n",
        "    x = rearrange(x, \"b s d -> b (s s1) d\")\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "class EEGConvEmbeddings(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_channels,\n",
        "        conv_channels,\n",
        "        kernel_size,\n",
        "        stride=1,\n",
        "        padding=0,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes the EEGConvEmbeddings module.\n",
        "\n",
        "        Args:\n",
        "        - num_channels (int): Number of EEG channels in the input data.\n",
        "        - conv_channels (int): Number of output channels for the convolutional layer.\n",
        "        - kernel_size (int): Size of the convolutional kernel.\n",
        "        - stride (int, optional): Stride of the convolution. Default: 1.\n",
        "        - padding (int, optional): Padding added to both sides of the input. Default: 0.\n",
        "        \"\"\"\n",
        "        super(EEGConvEmbeddings, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(\n",
        "            in_channels=num_channels,\n",
        "            out_channels=conv_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            stride=stride,\n",
        "            padding=padding,\n",
        "        )\n",
        "\n",
        "        # Additional layers and operations can be added here\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the EEGConvEmbeddings module.\n",
        "\n",
        "        Args:\n",
        "        - x (Tensor): Input tensor of shape (batch_size, num_channels, time_samples)\n",
        "\n",
        "        Returns:\n",
        "        - Tensor: Output tensor after convolution\n",
        "        \"\"\"\n",
        "        x = self.conv1(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FMRIEmbedding(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=1,\n",
        "        out_channels=32,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        padding=1,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes an fMRI Embedding Network.\n",
        "\n",
        "        Args:\n",
        "        - in_channels (int): Number of input channels (scans/modalities).\n",
        "        - out_channels (int): Number of output channels for the convolutional layer.\n",
        "        - kernel_size (int): Size of the convolutional kernels.\n",
        "        - stride (int): Stride of the convolutions.\n",
        "        - padding (int): Padding added to the input.\n",
        "\n",
        "        Example:\n",
        "        model = fMRIEmbeddingNet()\n",
        "        x = torch.randn(1, 1, 32, 32, 32)\n",
        "        input_tensor = torch.randn(8, 1, 64, 64, 64)  # 8 fMRI scans\n",
        "        output_tensor = model(input_tensor)\n",
        "        print(output_tensor.shape)  # torch.Size([8, 32, 64, 64, 64])\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        super(FMRIEmbedding, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv3d(\n",
        "            in_channels, out_channels, kernel_size, stride, padding\n",
        "        )\n",
        "        # Additional layers can be added here as needed\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the fMRI Embedding Network.\n",
        "\n",
        "        Args:\n",
        "        - x (Tensor): Input tensor of shape (batch_size, in_channels, D, H, W)\n",
        "\n",
        "        Returns:\n",
        "        - Tensor: Output embedding tensor\n",
        "        \"\"\"\n",
        "        x = self.conv1(x)\n",
        "        # Additional operations can be added here as needed\n",
        "        return x\n",
        "\n",
        "\n",
        "class MorpheusEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    MorpheusEncoder is a module that performs encoding on EEG data using multi-head attention and feed-forward networks.\n",
        "\n",
        "    Args:\n",
        "        dim (int): The dimension of the input data.\n",
        "        heads (int): The number of attention heads.\n",
        "        depth (int): The number of layers in the encoder.\n",
        "        dim_head (int): The dimension of each attention head.\n",
        "        dropout (int): The dropout rate.\n",
        "        num_channels (int): The number of input channels in the EEG data.\n",
        "        conv_channels (int): The number of output channels after the convolutional layer.\n",
        "        kernel_size (int): The size of the convolutional kernel.\n",
        "        stride (int, optional): The stride of the convolutional layer. Defaults to 1.\n",
        "        padding (int, optional): The padding size for the convolutional layer. Defaults to 0.\n",
        "        ff_mult (int, optional): The multiplier for the feed-forward network hidden dimension. Defaults to 4.\n",
        "\n",
        "    Attributes:\n",
        "        dim (int): The dimension of the input data.\n",
        "        heads (int): The number of attention heads.\n",
        "        depth (int): The number of layers in the encoder.\n",
        "        dim_head (int): The dimension of each attention head.\n",
        "        dropout (int): The dropout rate.\n",
        "        num_channels (int): The number of input channels in the EEG data.\n",
        "        conv_channels (int): The number of output channels after the convolutional layer.\n",
        "        kernel_size (int): The size of the convolutional kernel.\n",
        "        stride (int): The stride of the convolutional layer.\n",
        "        padding (int): The padding size for the convolutional layer.\n",
        "        ff_mult (int): The multiplier for the feed-forward network hidden dimension.\n",
        "        mha (MultiheadAttention): The multi-head attention module.\n",
        "        ffn (FeedForward): The feed-forward network module.\n",
        "        eeg_embedding (EEGConvEmbeddings): The EEG convolutional embedding module.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        heads: int,\n",
        "        depth: int,\n",
        "        dim_head: int,\n",
        "        dropout: int,\n",
        "        num_channels,\n",
        "        conv_channels,\n",
        "        kernel_size,\n",
        "        stride=1,\n",
        "        padding=0,\n",
        "        ff_mult: int = 4,\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super(MorpheusEncoder, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.heads = heads\n",
        "        self.depth = depth\n",
        "        self.dim_head = dim_head\n",
        "        self.dropout = dropout\n",
        "        self.num_channels = num_channels\n",
        "        self.conv_channels = conv_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.ff_mult = ff_mult\n",
        "\n",
        "        self.mha = nn.MultiheadAttention(\n",
        "            dim,\n",
        "            heads,\n",
        "            dropout\n",
        "        )\n",
        "\n",
        "        self.ffn = FeedForward(dim, dim, dropout, *args, **kwargs)\n",
        "\n",
        "        self.eeg_embedding = EEGConvEmbeddings(\n",
        "            num_channels, conv_channels, kernel_size, stride, padding\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of the MorpheusEncoder module.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): The input tensor of shape (batch_size, seq_len, dim).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The output tensor of shape (batch_size, seq_len, dim).\n",
        "\n",
        "        \"\"\"\n",
        "        x = self.eeg_embedding(x)\n",
        "        # print(x.shape)\n",
        "\n",
        "        x = self.mha(x, x, x) + x\n",
        "\n",
        "        x = self.ffn(x) + x\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class MorpheusDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    MorpheusDecoder is a module that performs decoding in the Morpheus model.\n",
        "\n",
        "    Args:\n",
        "        dim (int): The dimension of the input and output tensors.\n",
        "        heads (int): The number of attention heads.\n",
        "        depth (int): The number of layers in the decoder.\n",
        "        dim_head (int): The dimension of each attention head.\n",
        "        dropout (int): The dropout rate.\n",
        "        num_channels: The number of channels in the input tensor.\n",
        "        conv_channels: The number of channels in the convolutional layers.\n",
        "        kernel_size: The size of the convolutional kernel.\n",
        "        in_channels: The number of input channels for the FMRI embedding.\n",
        "        out_channels: The number of output channels for the FMRI embedding.\n",
        "        stride (int, optional): The stride of the convolutional layers. Defaults to 1.\n",
        "        padding (int, optional): The padding size for the convolutional layers. Defaults to 0.\n",
        "        ff_mult (int, optional): The multiplier for the feed-forward network dimension. Defaults to 4.\n",
        "\n",
        "    Attributes:\n",
        "        dim (int): The dimension of the input and output tensors.\n",
        "        heads (int): The number of attention heads.\n",
        "        depth (int): The number of layers in the decoder.\n",
        "        dim_head (int): The dimension of each attention head.\n",
        "        dropout (int): The dropout rate.\n",
        "        num_channels: The number of channels in the input tensor.\n",
        "        conv_channels: The number of channels in the convolutional layers.\n",
        "        kernel_size: The size of the convolutional kernel.\n",
        "        stride (int): The stride of the convolutional layers.\n",
        "        padding (int): The padding size for the convolutional layers.\n",
        "        ff_mult (int): The multiplier for the feed-forward network dimension.\n",
        "        frmi_embedding (nn.Linear): The linear layer for FRMI embedding.\n",
        "        masked_attn (MultiQueryAttention): The masked attention module.\n",
        "        mha (MultiheadAttention): The multihead attention module.\n",
        "        frmni_embedding (FMRIEmbedding): The FMRI embedding module.\n",
        "        ffn (FeedForward): The feed-forward network module.\n",
        "        proj (nn.Linear): The linear layer for projection to original dimension.\n",
        "        softmax (nn.Softmax): The softmax activation function.\n",
        "        encoder (MorpheusEncoder): The MorpheusEncoder module.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        heads: int,\n",
        "        depth: int,\n",
        "        dim_head: int,\n",
        "        dropout: int,\n",
        "        num_channels,\n",
        "        conv_channels,\n",
        "        kernel_size,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        stride=1,\n",
        "        padding=0,\n",
        "        ff_mult: int = 4,\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super(MorpheusDecoder, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.heads = heads\n",
        "        self.depth = depth\n",
        "        self.dim_head = dim_head\n",
        "        self.dropout = dropout\n",
        "        self.num_channels = num_channels\n",
        "        self.conv_channels = conv_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.ff_mult = ff_mult\n",
        "\n",
        "        self.frmi_embedding = nn.Linear(num_channels, dim)\n",
        "\n",
        "        self.masked_attn = MultiQueryAttention(\n",
        "            dim,\n",
        "            heads,\n",
        "        )\n",
        "\n",
        "        self.mha = nn.MultiheadAttention(\n",
        "            dim,\n",
        "            heads,\n",
        "            dropout\n",
        "        )\n",
        "\n",
        "        self.frmni_embedding = FMRIEmbedding(\n",
        "            in_channels, out_channels, kernel_size, stride, padding\n",
        "        )\n",
        "\n",
        "        self.ffn = FeedForward(dim, dim, dropout, *args, **kwargs)\n",
        "\n",
        "        self.proj = nn.Linear(dim, num_channels)\n",
        "\n",
        "        self.softmax = nn.Softmax(1)\n",
        "\n",
        "        self.encoder = MorpheusEncoder(\n",
        "            dim,\n",
        "            heads,\n",
        "            depth,\n",
        "            dim_head,\n",
        "            dropout,\n",
        "            num_channels,\n",
        "            conv_channels,\n",
        "            kernel_size,\n",
        "            stride,\n",
        "            padding,\n",
        "            ff_mult,\n",
        "            *args,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    def forward(self, frmi: Tensor, eeg: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Perform forward pass through the MorpheusDecoder.\n",
        "\n",
        "        Args:\n",
        "            frmi (Tensor): The FRMI input tensor.\n",
        "            eeg (Tensor): The EEG input tensor.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The output tensor after decoding.\n",
        "\n",
        "        \"\"\"\n",
        "        # X = FRMI of shapef\n",
        "        # # MRI data is represented as a 4D tensor: [batch_size, channels, depth, height, width].\n",
        "        # # EEG data is represented as a 3D tensor: [batch_size, channels, time_samples].\n",
        "        x = self.frmi_embedding(frmi)\n",
        "\n",
        "        # Rearrange to text dimension\n",
        "        x = reduce(x, \"b c d h w -> b (h w) (c d)\", \"sum\")\n",
        "\n",
        "        # Rearrange tensor to be compatible with attn\n",
        "        x = threed_to_text(x, self.num_channels, self.dim)\n",
        "\n",
        "        # Masked Attention\n",
        "        x, _, _ = self.masked_attn(x)\n",
        "\n",
        "        # EEG Encoder\n",
        "        eeg = self.encoder(eeg)\n",
        "\n",
        "        # Multihead Attention\n",
        "        x = self.mha(x, eeg, x) + x\n",
        "\n",
        "        # Feed Forward\n",
        "        x = self.ffn(x) + x\n",
        "\n",
        "        # Projection to original dimension\n",
        "        x = self.proj(x)\n",
        "\n",
        "        # Softmax\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Morpheus(nn.Module):\n",
        "    \"\"\"\n",
        "    Morpheus model implementation.\n",
        "\n",
        "    Args:\n",
        "        dim (int): Dimension of the model.\n",
        "        heads (int): Number of attention heads.\n",
        "        depth (int): Number of layers in the model.\n",
        "        dim_head (int): Dimension of each attention head.\n",
        "        dropout (int): Dropout rate.\n",
        "        num_channels: Number of input channels.\n",
        "        conv_channels: Number of channels in the convolutional layers.\n",
        "        kernel_size: Size of the convolutional kernel.\n",
        "        in_channels: Number of input channels for the convolutional layers.\n",
        "        out_channels: Number of output channels for the convolutional layers.\n",
        "        stride (int, optional): Stride value for the convolutional layers. Defaults to 1.\n",
        "        padding (int, optional): Padding value for the convolutional layers. Defaults to 0.\n",
        "        ff_mult (int, optional): Multiplier for the feed-forward layer dimension. Defaults to 4.\n",
        "\n",
        "    Attributes:\n",
        "        dim (int): Dimension of the model.\n",
        "        heads (int): Number of attention heads.\n",
        "        depth (int): Number of layers in the model.\n",
        "        dim_head (int): Dimension of each attention head.\n",
        "        dropout (int): Dropout rate.\n",
        "        num_channels: Number of input channels.\n",
        "        conv_channels: Number of channels in the convolutional layers.\n",
        "        kernel_size: Size of the convolutional kernel.\n",
        "        stride (int): Stride value for the convolutional layers.\n",
        "        padding (int): Padding value for the convolutional layers.\n",
        "        ff_mult (int): Multiplier for the feed-forward layer dimension.\n",
        "        layers (nn.ModuleList): List of MorpheusDecoder layers.\n",
        "        norm (nn.LayerNorm): Layer normalization module.\n",
        "\n",
        "    Examples:\n",
        "        >>> model = Morpheus(\n",
        "        ...     dim=128,\n",
        "        ...     heads=4,\n",
        "        ...     depth=2,\n",
        "        ...     dim_head=32,\n",
        "        ...     dropout=0.1,\n",
        "        ...     num_channels=32,\n",
        "        ...     conv_channels=32,\n",
        "        ...     kernel_size=3,\n",
        "        ...     in_channels=1,\n",
        "        ...     out_channels=32,\n",
        "        ...     stride=1,\n",
        "        ...     padding=1,\n",
        "        ...     ff_mult=4,\n",
        "        ... )\n",
        "        >>> frmi = torch.randn(1, 1, 32, 32, 32)\n",
        "        >>> eeg = torch.randn(1, 32, 128)\n",
        "        >>> output = model(frmi, eeg)\n",
        "        >>> print(output.shape)\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        heads: int,\n",
        "        depth: int,\n",
        "        dim_head: int,\n",
        "        dropout: int,\n",
        "        num_channels,\n",
        "        conv_channels,\n",
        "        kernel_size,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        stride=1,\n",
        "        padding=0,\n",
        "        ff_mult: int = 4,\n",
        "        scatter: bool = False,\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super(Morpheus, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.heads = heads\n",
        "        self.depth = depth\n",
        "        self.dim_head = dim_head\n",
        "        self.dropout = dropout\n",
        "        self.num_channels = num_channels\n",
        "        self.conv_channels = conv_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.ff_mult = ff_mult\n",
        "        self.scatter = scatter\n",
        "\n",
        "        self.layers = nn.ModuleList([])\n",
        "\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(\n",
        "                MorpheusDecoder(\n",
        "                    dim,\n",
        "                    heads,\n",
        "                    depth,\n",
        "                    dim_head,\n",
        "                    dropout,\n",
        "                    num_channels,\n",
        "                    conv_channels,\n",
        "                    kernel_size,\n",
        "                    in_channels,\n",
        "                    out_channels,\n",
        "                    stride,\n",
        "                    padding,\n",
        "                    ff_mult,\n",
        "                    *args,\n",
        "                    **kwargs,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.norm = nn.LayerNorm(num_channels)\n",
        "\n",
        "    def forward(self, frmi: Tensor, eeg: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of the Morpheus model.\n",
        "\n",
        "        Args:\n",
        "            frmi (Tensor): Input tensor for the frmi modality.\n",
        "            eeg (Tensor): Input tensor for the eeg modality.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Output tensor of the model.\n",
        "\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(frmi, eeg)\n",
        "            x = self.norm(x)\n",
        "\n",
        "        if self.scatter:\n",
        "            # Scatter the tensor to 4d spatial tensor\n",
        "            s1 = x.shape[1]\n",
        "            x = rearrange(x, \"b (s s1) d -> b s s1 d\", s1=s1)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "ZZBkjJLZ5axD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Morpheus(\n",
        "    dim=128,  # Dimension of the model\n",
        "    heads=4,  # Number of attention heads\n",
        "    depth=2,  # Number of transformer layers\n",
        "    dim_head=32,  # Dimension of each attention head\n",
        "    dropout=0.1,  # Dropout rate\n",
        "    num_channels=32,  # Number of input channels\n",
        "    conv_channels=32,  # Number of channels in convolutional layers\n",
        "    kernel_size=3,  # Kernel size for convolutional layers\n",
        "    in_channels=1,  # Number of input channels for convolutional layers\n",
        "    out_channels=32,  # Number of output channels for convolutional layers\n",
        "    stride=1,  # Stride for convolutional layers\n",
        "    padding=1,  # Padding for convolutional layers\n",
        "    ff_mult=4,  # Multiplier for feed-forward layer dimension\n",
        "    scatter = False, # Whether to scatter to 4d representing spatial dimensions\n",
        ")\n",
        "\n",
        "# Creating random tensors for input data\n",
        "frmi = torch.randn(1, 1, 32, 32, 32)  # Random tensor for FRMI data\n",
        "eeg = torch.randn(1, 32, 128)  # Random tensor for EEG data\n",
        "\n",
        "# Passing the input data through the model to get the output\n",
        "output = model(frmi, eeg)\n",
        "\n",
        "# Printing the shape of the output tensor\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RacSAKsZ72ps",
        "outputId": "0c554a8f-fb78-4343-a85d-5e83f679f068"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "can only concatenate tuple (not \"Tensor\") to tuple",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-71fc227889db>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Passing the input data through the model to get the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrmi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meeg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Printing the shape of the output tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-dc9512cc24e0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, frmi, eeg)\u001b[0m\n\u001b[1;32m    542\u001b[0m         \"\"\"\n\u001b[1;32m    543\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrmi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meeg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-dc9512cc24e0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, frmi, eeg)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;31m# EEG Encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0meeg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meeg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0;31m# Multihead Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-dc9512cc24e0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can only concatenate tuple (not \"Tensor\") to tuple"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MorpheusDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    MorpheusDecoder is a module that performs decoding in the Morpheus model.\n",
        "\n",
        "    Args:\n",
        "        dim (int): The dimension of the input and output tensors.\n",
        "        heads (int): The number of attention heads.\n",
        "        depth (int): The number of layers in the decoder.\n",
        "        dim_head (int): The dimension of each attention head.\n",
        "        dropout (int): The dropout rate.\n",
        "        num_channels: The number of channels in the input tensor.\n",
        "        conv_channels: The number of channels in the convolutional layers.\n",
        "        kernel_size: The size of the convolutional kernel.\n",
        "        in_channels: The number of input channels for the FMRI embedding.\n",
        "        out_channels: The number of output channels for the FMRI embedding.\n",
        "        stride (int, optional): The stride of the convolutional layers. Defaults to 1.\n",
        "        padding (int, optional): The padding size for the convolutional layers. Defaults to 0.\n",
        "        ff_mult (int, optional): The multiplier for the feed-forward network dimension. Defaults to 4.\n",
        "\n",
        "    Attributes:\n",
        "        dim (int): The dimension of the input and output tensors.\n",
        "        heads (int): The number of attention heads.\n",
        "        depth (int): The number of layers in the decoder.\n",
        "        dim_head (int): The dimension of each attention head.\n",
        "        dropout (int): The dropout rate.\n",
        "        num_channels: The number of channels in the input tensor.\n",
        "        conv_channels: The number of channels in the convolutional layers.\n",
        "        kernel_size: The size of the convolutional kernel.\n",
        "        stride (int): The stride of the convolutional layers.\n",
        "        padding (int): The padding size for the convolutional layers.\n",
        "        ff_mult (int): The multiplier for the feed-forward network dimension.\n",
        "        frmi_embedding (nn.Linear): The linear layer for FRMI embedding.\n",
        "        masked_attn (MultiQueryAttention): The masked attention module.\n",
        "        mha (MultiheadAttention): The multihead attention module.\n",
        "        frmni_embedding (FMRIEmbedding): The FMRI embedding module.\n",
        "        ffn (FeedForward): The feed-forward network module.\n",
        "        proj (nn.Linear): The linear layer for projection to original dimension.\n",
        "        softmax (nn.Softmax): The softmax activation function.\n",
        "        encoder (MorpheusEncoder): The MorpheusEncoder module.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        heads: int,\n",
        "        depth: int,\n",
        "        dim_head: int,\n",
        "        dropout: int,\n",
        "        num_channels,\n",
        "        conv_channels,\n",
        "        kernel_size,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        stride=1,\n",
        "        padding=0,\n",
        "        ff_mult: int = 4,\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super(MorpheusDecoder, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.heads = heads\n",
        "        self.depth = depth\n",
        "        self.dim_head = dim_head\n",
        "        self.dropout = dropout\n",
        "        self.num_channels = num_channels\n",
        "        self.conv_channels = conv_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.ff_mult = ff_mult\n",
        "\n",
        "        self.frmi_embedding = nn.Linear(num_channels, dim)\n",
        "\n",
        "        self.masked_attn = MultiQueryAttention(\n",
        "            dim,\n",
        "            heads,\n",
        "        )\n",
        "\n",
        "        self.frmni_embedding = FMRIEmbedding(\n",
        "            in_channels, out_channels, kernel_size, stride, padding\n",
        "        )\n",
        "\n",
        "        self.ffn = FeedForward(dim, dim, dropout, *args, **kwargs)\n",
        "\n",
        "        self.proj = nn.Linear(dim, num_channels)\n",
        "\n",
        "        self.softmax = nn.Softmax(1)\n",
        "\n",
        "    def forward(self, frmi: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Perform forward pass through the MorpheusDecoder.\n",
        "\n",
        "        Args:\n",
        "            frmi (Tensor): The FRMI input tensor.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The output tensor after decoding.\n",
        "\n",
        "        \"\"\"\n",
        "        # X = FRMI of shapef\n",
        "        # # MRI data is represented as a 4D tensor: [batch_size, channels, depth, height, width].\n",
        "        x = self.frmi_embedding(frmi)\n",
        "\n",
        "        # Rearrange to text dimension\n",
        "        x = reduce(x, \"b c d h w -> b (h w) (c d)\", \"sum\")\n",
        "\n",
        "        # Rearrange tensor to be compatible with attn\n",
        "        x = threed_to_text(x, self.num_channels, self.dim)\n",
        "\n",
        "        # Masked Attention\n",
        "        x, _, _ = self.masked_attn(x)\n",
        "\n",
        "        # Feed Forward\n",
        "        x = self.ffn(x) + x\n",
        "\n",
        "        # Projection to original dimension\n",
        "        x = self.proj(x)\n",
        "\n",
        "        # Softmax\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "PsopuJO_-qUL"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = MorpheusDecoder(\n",
        "    dim=128,  # Dimension of the model\n",
        "    heads=4,  # Number of attention heads\n",
        "    depth=2,  # Number of transformer layers\n",
        "    dim_head=32,  # Dimension of each attention head\n",
        "    dropout=0.1,  # Dropout rate\n",
        "    num_channels=32,  # Number of input channels\n",
        "    conv_channels=32,  # Number of channels in convolutional layers\n",
        "    kernel_size=3,  # Kernel size for convolutional layers\n",
        "    in_channels=1,  # Number of input channels for convolutional layers\n",
        "    out_channels=32,  # Number of output channels for convolutional layers\n",
        "    stride=1,  # Stride for convolutional layers\n",
        "    padding=1,  # Padding for convolutional layers\n",
        "    ff_mult=4,  # Multiplier for feed-forward layer dimension\n",
        ")\n",
        "\n",
        "# Creating random tensors for input data\n",
        "frmi = torch.randn(1, 1, 32, 32, 32)  # Random tensor for FRMI data\n",
        "\n",
        "# Passing the input data through the model to get the output\n",
        "output = decoder(frmi)\n",
        "\n",
        "# Printing the shape of the output tensor\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cIiZFKJ-WiS",
        "outputId": "0fbd2e7e-296d-42b1-9973-11a520d8031c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nilearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PpLEcMH_yzM",
        "outputId": "bcfc4ee0-a69f-48fe-e126-2dbf75c491ee"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nilearn\n",
            "  Downloading nilearn-0.10.3-py3-none-any.whl (10.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.3.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nilearn) (4.9.4)\n",
            "Requirement already satisfied: nibabel>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (4.0.2)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nilearn) (23.2)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (2.31.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.11.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nibabel>=4.0.0->nilearn) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->nilearn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->nilearn) (2023.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (2023.11.17)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->nilearn) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->nilearn) (1.16.0)\n",
            "Installing collected packages: nilearn\n",
            "Successfully installed nilearn-0.10.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nilearn import datasets\n",
        "from nilearn.datasets import load_mni152_template\n",
        "from nilearn.image import resample_to_img\n",
        "from nilearn import masking\n",
        "from nilearn import image as nimg\n",
        "from nilearn import plotting\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.linear_model import MultiTaskLasso"
      ],
      "metadata": {
        "id": "l_UfUErf_0sZ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adhd_dataset = datasets.fetch_adhd(n_subjects=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pf1P9EnN_2no",
        "outputId": "5a296218-7c87-421e-9a4d-ca86bc47f90a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Added README.md to /root/nilearn_data\n",
            "\n",
            "\n",
            "Dataset created in /root/nilearn_data/adhd\n",
            "\n",
            "Downloading data from https://www.nitrc.org/frs/download.php/7781/adhd40_metadata.tgz ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " ...done. (1 seconds, 0 min)\n",
            "Extracting data from /root/nilearn_data/adhd/fbef5baff0b388a8c913a08e1d84e059/adhd40_metadata.tgz..... done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.nitrc.org/frs/download.php/7782/adhd40_0010042.tgz ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloaded 23175168 of 44414948 bytes (52.2%,    0.9s remaining) ...done. (2 seconds, 0 min)\n",
            "Extracting data from /root/nilearn_data/adhd/31769c9cee5cd55f045e62633d651f3d/adhd40_0010042.tgz..... done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.nitrc.org/frs/download.php/7783/adhd40_0010064.tgz ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloaded 43098112 of 45583539 bytes (94.5%,    0.1s remaining) ...done. (1 seconds, 0 min)\n",
            "Extracting data from /root/nilearn_data/adhd/31769c9cee5cd55f045e62633d651f3d/adhd40_0010064.tgz..... done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adhd_raw_data = nimg.get_data(nimg.load_img(adhd_dataset.func[0]))"
      ],
      "metadata": {
        "id": "u4KfIQNSA6D0"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(adhd_raw_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuOvnlC_A4jv",
        "outputId": "6b03f43e-bf66-48e0-f0df-1f9f8fceb3dc"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(61, 73, 61, 176)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adhd_raw_data = rearrange(adhd_raw_data, \"x y z t -> t z y x\")"
      ],
      "metadata": {
        "id": "LLrmMIC4BfsQ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(adhd_raw_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxktZK5sCvsH",
        "outputId": "db398b90-508f-4915-dd66-50d06a58ca1a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(176, 61, 73, 61)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adhd_raw_data = adhd_raw_data.reshape(1, 176, 61, 73, 61)\n",
        "adhd_raw_data = torch.Tensor(adhd_raw_data)"
      ],
      "metadata": {
        "id": "ew5BB3TMCjiO"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = MorpheusDecoder(\n",
        "    dim=128,  # Dimension of the model\n",
        "    heads=4,  # Number of attention heads\n",
        "    depth=2,  # Number of transformer layers\n",
        "    dim_head=32,  # Dimension of each attention head\n",
        "    dropout=0.1,  # Dropout rate\n",
        "    num_channels=61,  # Number of input channels\n",
        "    conv_channels=32,  # Number of channels in convolutional layers\n",
        "    kernel_size=3,  # Kernel size for convolutional layers\n",
        "    in_channels=1,  # Number of input channels for convolutional layers\n",
        "    out_channels=32,  # Number of output channels for convolutional layers\n",
        "    stride=1,  # Stride for convolutional layers\n",
        "    padding=1,  # Padding for convolutional layers\n",
        "    ff_mult=4,  # Multiplier for feed-forward layer dimension\n",
        ")\n",
        "\n",
        "# Passing the input data through the model to get the output\n",
        "output = decoder(adhd_raw_data)\n",
        "\n",
        "# Printing the shape of the output tensor\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfMjYGdzC1nA",
        "outputId": "815d3a57-dbb2-489b-f5ca-0d2813a8c60e"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 61, 61])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhEnvqIKDUod",
        "outputId": "32bf9e95-2c00-49cb-e1da-4d9eb4883c8d"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.0000e+00, 0.0000e+00, 1.0000e+00,  ..., 0.0000e+00,\n",
              "          0.0000e+00, 0.0000e+00],\n",
              "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "          0.0000e+00, 0.0000e+00],\n",
              "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "          0.0000e+00, 0.0000e+00],\n",
              "         ...,\n",
              "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "          0.0000e+00, 2.8178e-01],\n",
              "         [1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.0594e-26,\n",
              "          0.0000e+00, 0.0000e+00],\n",
              "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "          1.2752e-43, 0.0000e+00]]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    }
  ]
}